{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  ../../RESULTS/FULLEXPERIMENT/r100tc-100aINTERg0.99  Created \n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import itertools as it\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from insect_rl.mdp.utils import grid_math\n",
    "from insect_rl.mdp.mrl import MyGridWorld, IAVICallback\n",
    "\n",
    "from mushroom_rl.core import Core\n",
    "import mushroom_rl.utils.dataset as mrl_dataset\n",
    "from mushroom_rl.utils.callbacks import CollectDataset\n",
    "\n",
    "if \"../workflow/scripts\" not in sys.path:\n",
    "    sys.path.insert(0, os.path.abspath(\"../scripts/\"))\n",
    "import exp_funs\n",
    "\n",
    "#CONDITIONS = [\"NOTRAP\",\"TRAP\"]\n",
    "REWARD = 100\n",
    "TRAP_COST = -100\n",
    "\n",
    "NUM_AGENTS = 1\n",
    "\n",
    "\n",
    "with open(\"../../temp/agent.pickle\", 'rb') as agent_i:\n",
    "    agent = pickle.load(agent_i)\n",
    "with open(\"../../Wystrach2020/env.pickle\", 'rb') as env_i:\n",
    "    env_settings = pickle.load(env_i)\n",
    "\n",
    "\n",
    "sim_settings = {\n",
    "    'reward':REWARD,\n",
    "    'trap_cost':TRAP_COST,\n",
    "    'actions':vars(grid_math)[\"INTERCARDINALS\"],\n",
    "    'gamma':.99\n",
    "}\n",
    "res_dir = f\"../../RESULTS/FULLEXPERIMENT/r{sim_settings['reward']}tc{sim_settings['trap_cost']}aINTERg{sim_settings['gamma']}\"\n",
    "try:\n",
    "    # Create target Directory\n",
    "    os.mkdir(res_dir)\n",
    "    print(\"Directory \" , res_dir ,  \" Created \") \n",
    "except FileExistsError:\n",
    "    print(\"Directory \" , res_dir ,  \" already exists\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid(data):\n",
    "    x, y = zip(*data)\n",
    "    l = len(x)\n",
    "    return int(round(sum(x) / l)), int(round(sum(y) / l))\n",
    "\n",
    "def environment(configs, sim_settings):\n",
    "    goals = configs.pop(\"goals\")\n",
    "    width = configs[\"width\"]\n",
    "    \n",
    "    configs[\"width\"] = configs[\"height\"] # TODO WHYYY MUSHROOM_RL???\n",
    "    configs[\"height\"] = width\n",
    "\n",
    "    #configs[\"goal\"] = [(g[1], g[0]) for g in goals] # TODO only one possible\n",
    "    configs[\"goal\"] = centroid(goals)\n",
    "\n",
    "    return MyGridWorld(**(configs | sim_settings))\n",
    "\n",
    "\n",
    "mdp = environment(env_settings, sim_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating the agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = [agent(mdp.info) for i in range(NUM_AGENTS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<mushroom_rl.algorithms.value.td.sarsa.SARSA at 0x7f3cf2b422c0>,\n",
       " <mushroom_rl.algorithms.value.td.sarsa.SARSA at 0x7f3bf556a710>,\n",
       " <mushroom_rl.algorithms.value.td.sarsa.SARSA at 0x7f3c100a1ba0>,\n",
       " <mushroom_rl.algorithms.value.td.sarsa.SARSA at 0x7f3ce0fcb070>,\n",
       " <mushroom_rl.algorithms.value.td.sarsa.SARSA at 0x7f3ce0fca6b0>,\n",
       " <mushroom_rl.algorithms.value.td.sarsa.SARSA at 0x7f3ce0fcb400>,\n",
       " <mushroom_rl.algorithms.value.td.sarsa.SARSA at 0x7f3ce0fcbd00>,\n",
       " <mushroom_rl.algorithms.value.td.sarsa.SARSA at 0x7f3ce0fcb910>,\n",
       " <mushroom_rl.algorithms.value.td.sarsa.SARSA at 0x7f3ce0fca050>,\n",
       " <mushroom_rl.algorithms.value.td.sarsa.SARSA at 0x7f3ce0fcb730>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../../temp/agent.pickle\", 'rb') as agent_i:\n",
    "    agent_cons = pickle.load(agent_i)\n",
    "agents = [agent] + [agent_cons(mdp.info) for i in range(9)]\n",
    "agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(mdp, condition, agent, i_agent):\n",
    "    result_dir = f\"{res_dir}/{condition}/a{i_agent}\"\n",
    "    os.mkdir(result_dir)\n",
    "    collect_dataset = CollectDataset()\n",
    "    iavi_dataconverter = IAVICallback(mdp, agent, statement=\"fit\")\n",
    "\n",
    "    callbacks = [collect_dataset, iavi_dataconverter]\n",
    "    core = Core(agent, mdp)\n",
    "    core.callbacks_fit = callbacks\n",
    "\n",
    "\n",
    "    len_batch = min(ITERATIONS, 10)\n",
    "\n",
    "    its = list(range(0, ITERATIONS, len_batch))\n",
    "    data = []\n",
    "    Js = []\n",
    "    episode_lens = []\n",
    "\n",
    "    tds = np.zeros(iavi_dataconverter.cum_td.shape)\n",
    "    tds_ns = np.zeros(iavi_dataconverter.cum_td.shape)\n",
    "    for i in range(len(its)):\n",
    "        #print(f\"batch {i}, {len(data)}\")\n",
    "        core.learn(n_episodes=len_batch, n_steps_per_fit=1, quiet=False)\n",
    "        training_dataset = collect_dataset.get()\n",
    "        data.extend(training_dataset)\n",
    "        collect_dataset.clean()\n",
    "        Js.append(exp_funs.compute_metrics(training_dataset, mdp.info.gamma))\n",
    "        episode_lens.extend(mrl_dataset.episodes_length(training_dataset))\n",
    "\n",
    "        for i in range(iavi_dataconverter.cum_td.shape[0]):\n",
    "            # TODO maybe not sum but average?\n",
    "            tds[i] += iavi_dataconverter.cum_td[np.array([i])]\n",
    "            tds_ns[i] += iavi_dataconverter.cum_td_ns[np.array([i])]\n",
    "\n",
    "    Js = list(it.chain.from_iterable(Js))\n",
    "    with open(f\"{result_dir}/J_{i_agent}.pickle\", 'wb') as o:\n",
    "        pickle.dump(Js, o)\n",
    "    \n",
    "    with open(f\"{result_dir}/episode_lens_{i_agent}.pickle\", 'wb') as o:\n",
    "        pickle.dump(episode_lens, o)\n",
    "\n",
    "    shape = iavi_dataconverter.V.shape\n",
    "    v = np.zeros(shape)\n",
    "    for i in range(shape[0]):\n",
    "        v[i] = iavi_dataconverter.V[np.array([i])]\n",
    "\n",
    "    np.save(f\"{result_dir}/value_fun_{i_agent}.npy\", np.rot90(v.reshape(mdp._height, mdp._width)))\n",
    "    np.save(f\"{result_dir}/tds_{i_agent}.npy\", np.rot90(tds.reshape(mdp._height, mdp._width)))\n",
    "    np.save(f\"{result_dir}/tds_ns_{i_agent}.npy\", np.rot90(tds_ns.reshape(mdp._height, mdp._width)))\n",
    "\n",
    "    shape = agent.Q.shape\n",
    "    q = np.zeros(shape)\n",
    "    for i in range(shape[0]):\n",
    "        for j in range(shape[1]):\n",
    "            state = np.array([i])\n",
    "            action = np.array([j])\n",
    "            q[i, j] = agent.Q.predict(state, action)\n",
    "    np.save(f\"{result_dir}/q_{i_agent}.npy\", q)\n",
    "\n",
    "    with open(f\"{result_dir}/data_{i_agent}.pickle\", 'wb') as o:\n",
    "        pickle.dump(data, o)\n",
    "    df = exp_funs.convert_trajectories(data, mdp)\n",
    "    df.to_csv(f\"{result_dir}/df_{i_agent}.csv\")\n",
    "    agent.save(f\"{result_dir}/agent_{i_agent}\", full_save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1 (with the trap closed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  ../../RESULTS/FULLEXPERIMENT/r100tc-100aINTERg0.99/0-no-trap  already exists\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paulina/anaconda3/envs/snakemake/lib/python3.10/site-packages/numpy/core/fromnumeric.py:43: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  result = getattr(asarray(obj), method)(*args, **kwds)\n",
      "                                               \r"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "condition = \"0-no-trap\"\n",
    "ITERATIONS = 1000\n",
    "\n",
    "try:\n",
    "    # Create target Directory\n",
    "    os.mkdir(dir_:=f\"{res_dir}/{condition}\")\n",
    "    print(\"Directory \" , dir_ ,  \" Created \") \n",
    "except FileExistsError:\n",
    "    print(\"Directory \" , dir_ ,  \" already exists\")\n",
    "\n",
    "for i_agent, agent in enumerate(agents[1:]):\n",
    "    print(i_agent + 1)\n",
    "    run_experiment(mdp, condition, agent, i_agent + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  ../../RESULTS/FULLEXPERIMENT/r100tc-100aINTERg0.99/1-trap  Created \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paulina/anaconda3/envs/snakemake/lib/python3.10/site-packages/numpy/core/fromnumeric.py:43: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  result = getattr(asarray(obj), method)(*args, **kwds)\n",
      "                                               \r"
     ]
    }
   ],
   "source": [
    "condition = \"1-trap\"\n",
    "ITERATIONS = 1000\n",
    "\n",
    "try:\n",
    "    # Create target Directory\n",
    "    os.mkdir(dir_:=f\"{res_dir}/{condition}\")\n",
    "    print(\"Directory \" , dir_ ,  \" Created \") \n",
    "except FileExistsError:\n",
    "    print(\"Directory \" , dir_ ,  \" already exists\")\n",
    "\n",
    "mdp.open_trap()\n",
    "\n",
    "for i_agent, agent in enumerate(agents[1:]):\n",
    "    run_experiment(mdp, condition, agent, i_agent + 1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4680b050f011275efdf0b426c72a88bda4539fa6d4243d99352204b96b569166"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('snakemake')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
